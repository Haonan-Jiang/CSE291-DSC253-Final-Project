{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import ollama\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "texts = dataset[\"train\"][\"text\"]\n",
    "labels = dataset[\"train\"][\"label\"]\n",
    "mapping = {0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\"}\n",
    "# shuffle the data\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "shuffled_indices = list(range(len(texts)))\n",
    "random.shuffle(shuffled_indices)\n",
    "\n",
    "texts = [texts[i] for i in shuffled_indices]\n",
    "\n",
    "labels = [labels[i] for i in shuffled_indices]\n",
    "# create a balanced dataset of size 10000\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "balanced_texts = []\n",
    "balanced_labels = []\n",
    "counter = Counter(labels)\n",
    "samples_per_label = 2500\n",
    "for i in range(4):\n",
    "    label = i\n",
    "    label_indices = np.where(np.array(labels) == label)[0]\n",
    "    label_indices = label_indices[:samples_per_label]\n",
    "    balanced_texts.extend([texts[i] for i in label_indices])\n",
    "    balanced_labels.extend([labels[i] for i in label_indices])\n",
    "\n",
    "# shuffle the balanced dataset\n",
    "shuffled_indices = list(range(len(balanced_texts)))\n",
    "random.shuffle(shuffled_indices)\n",
    "balanced_texts = [balanced_texts[i] for i in shuffled_indices]\n",
    "balanced_labels = [balanced_labels[i] for i in shuffled_indices]\n",
    "texts = balanced_texts\n",
    "labels = balanced_labels\n",
    "\n",
    "# verify the balance\n",
    "counter = Counter(balanced_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hewlett-Packard takes a new approach to software The company extends a deal with open-source Java software maker JBoss in an effort to compete with IBM.', 'Hurricanes Likely To Cause Insurers #39; Rates To Rise MIAMI -- Four hurricanes in just six weeks are likely going to force Florida #39;s top two insurers to raise rates or reduce their business in the state.', 'Three pops and Thomson goes out Atlanta Braves starting pitcher John Thomson, center, is pulled from the game against the Houston Astros during the first inning of Game 3 of the National League Division Series in Houston, on Saturday.']\n",
      "[3, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "print(texts[:3])\n",
    "print(labels[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Samples:\n",
      "Here are five sample news articles that challenge the classification accuracy:\n",
      "\n",
      "**Sample 1:**\n",
      "Title: \"NATO Leaders Meet to Discuss Global Security Threats\"\n",
      "Text: \"Leaders from NATO member countries gathered in Brussels yesterday to discuss the growing threat of cyber attacks on global infrastructure. The meeting came after a series of high-profile hacks targeted major financial institutions and government agencies. While the focus was on cybersecurity, some delegates also touched on issues related to climate change and sustainable energy.\"\n",
      "\n",
      "Category Challenge: This article blends international relations (World) with technology and security (Sci/Tech), making it difficult to categorize solely as one label.\n",
      "\n",
      "**Sample 2:**\n",
      "Title: \"Serena Williams Dominates at Australian Open, Eyes Grand Slam Record\"\n",
      "Text: \"Tennis superstar Serena Williams continued her remarkable comeback yesterday, defeating her opponent in straight sets to move closer to the record for most Grand Slam titles won by a female player. The victory marked her 23rd major championship win and left many speculating about her chances of surpassing Margaret Court's all-time record.\"\n",
      "\n",
      "Category Challenge: This article is primarily about sports (Sports), but also touches on women's empowerment and record-breaking, which could be classified under Sci/Tech or World.\n",
      "\n",
      "**Sample 3:**\n",
      "Title: \"Elon Musk's SpaceX to Send Private Mission to Mars\"\n",
      "Text: \"In a major breakthrough for private space exploration, Elon Musk's SpaceX announced plans to send a privately funded mission to Mars in the next few years. The mission aims to test new technologies and pave the way for potential human settlements on the red planet.\"\n",
      "\n",
      "Category Challenge: This article combines business (SpaceX is a company) with science and technology (Mars exploration), making it difficult to categorize solely as one label.\n",
      "\n",
      "**Sample 4:**\n",
      "Title: \"Global Economy Faces Uncertainty as Trade Tensions Escalate\"\n",
      "Text: \"Tensions between major trading nations have reached a boiling point, sparking concerns about the global economy's stability. A trade war could lead to widespread job losses and economic downturns, making it essential for governments to negotiate a resolution before things get out of hand.\"\n",
      "\n",
      "Category Challenge: This article is primarily about business (global economy) but also touches on international relations (World), making it difficult to categorize solely as one label.\n",
      "\n",
      "**Sample 5:**\n",
      "Title: \"New Study Reveals Surprising Connection Between Exercise and Brain Health\"\n",
      "Text: \"A groundbreaking study published today in a leading scientific journal reveals that regular exercise can significantly improve brain health, reducing the risk of dementia and cognitive decline. The findings have far-reaching implications for public health policy and individual well-being.\"\n",
      "\n",
      "Category Challenge: This article is primarily about science (study) but also touches on sports (exercise) and business (public health policy), making it difficult to categorize solely as one label.\n",
      "\n",
      "These samples are designed to challenge the classification accuracy by combining elements from multiple categories, requiring careful consideration of the context and themes within each article.\n",
      "Analysis of Samples:\n",
      "Based on the provided samples, I have derived general principles for classifying news articles into categories 'World', 'Sports', 'Business', or 'Sci/Tech':\n",
      "\n",
      "**General Characteristics:**\n",
      "\n",
      "* **'World':**\n",
      "\t+ Typically involves international relations, politics, or global issues.\n",
      "\t+ May touch on economic, social, and environmental topics related to global affairs.\n",
      "\t+ Often features government agencies, international organizations, or leaders as key players.\n",
      "* **'Sports':**\n",
      "\t+ Primarily focuses on athletic events, teams, or individuals.\n",
      "\t+ Typically involves competition, records, or achievements in various sports.\n",
      "\t+ May touch on issues related to athletes' personal lives, sponsorships, or controversies.\n",
      "* **'Business':**\n",
      "\t+ Usually centers around economic topics, companies, or industries.\n",
      "\t+ May involve financial news, market trends, or business strategies.\n",
      "\t+ Often features entrepreneurs, CEOs, or company leaders as key players.\n",
      "* **'Sci/Tech':**\n",
      "\t+ Typically involves scientific research, technological advancements, or innovations.\n",
      "\t+ May touch on health, environmental, or social implications of new discoveries or technologies.\n",
      "\t+ Often features experts, researchers, or organizations driving the development of new knowledge or products.\n",
      "\n",
      "**Common Mistakes and Guidelines:**\n",
      "\n",
      "* **Avoid misclassification due to peripheral mentions:**\n",
      "\t+ A story about a company's financial performance may not solely be classified as 'Business' if it also touches on environmental concerns (Sci/Tech) or social responsibility initiatives (World).\n",
      "* **Consider the primary focus:** If an article primarily discusses international relations, politics, or global issues, categorize it under 'World'. If the main focus is on a sports event, team, or individual, categorize it as 'Sports'.\n",
      "* **Be cautious of crossover topics:**\n",
      "\t+ A story about climate change may be classified as both 'Sci/Tech' (research and technology) and 'World' (global implications).\n",
      "\t+ An article discussing the economic impact of a natural disaster should be categorized under 'Business' rather than 'World', despite the global nature of the event.\n",
      "* **Apply these guidelines universally:** Classify news articles based on their primary focus, avoiding misclassification due to peripheral mentions or crossover topics.\n",
      "\n",
      "By following these general principles and guidelines, you can accurately predict the category of a news article without additional context.\n",
      "Optimized Prompt:\n",
      "Classify a news article as 'World', 'Sports', 'Business', or 'Sci/Tech' based on its primary focus. Consider the following criteria:\n",
      "\n",
      "* If the article primarily discusses international relations, politics, or global issues, categorize it under 'World'.\n",
      "* If the main focus is on a sports event, team, or individual, categorize it as 'Sports'.\n",
      "* If the article centers around economic topics, companies, or industries, and does not involve environmental concerns (Sci/Tech) or social responsibility initiatives (World), classify it under 'Business'.\n",
      "* If the article involves scientific research, technological advancements, or innovations, and does not primarily focus on international relations (World), sports events (Sports), or economic topics (Business), categorize it as 'Sci/Tech'.\n",
      "* Avoid misclassification due to peripheral mentions by focusing on the primary topic.\n",
      "* Be cautious of crossover topics and consider multiple criteria simultaneously.\n",
      "\n",
      "Respond with one of the following labels: 'World', 'Sports', 'Business', or 'Sci/Tech'.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Function to interact with the Ollama API\n",
    "def chat_with_context(history):\n",
    "    response = ollama.chat(model='llama3', messages=history)\n",
    "    return response['message']['content']\n",
    "\n",
    "# Initialize the chat history\n",
    "chat_history = []\n",
    "num_samples = 5\n",
    "\n",
    "# Step 1: Define Task Description for AG News Dataset\n",
    "task_description = \"We need to predict the category of a given news article. The labels are 'World', 'Sports', 'Business', and 'Sci/Tech'.\"\n",
    "\n",
    "generate_samples_request = (\n",
    "    f\"As an advanced language exposed to diverse datasets, you are expected to create {num_samples} samples for the task outlined below.\\n\"\n",
    "    \"Generate samples that are likely to be correctly classified as 'World', 'Sports', 'Business', or 'Sci/Tech', as well as samples that might challenge the classification accuracy according to the task instructions.\\n\\n\"\n",
    "    f\"### Task Description:\\n{task_description}\\n\\n\"\n",
    "    \"### Requirements for Samples:\\n\"\n",
    "    \"1. Each sample must present a unique and intricate challenge.\\n\"\n",
    "    \"2. The complexity of the samples should be such that simply applying the given task instruction would likely lead to incorrect or incomplete results for some samples.\\n\"\n",
    "    \"3. The samples should cover a diverse range of scenarios within the scope of the task, avoiding repetition and predictability.\\n\"\n",
    "    \"4. Ensure that the samples, while challenging, remain realistic and pertinent to the task's context.\\n\"\n",
    "    \"Generate the samples keeping these requirements in mind.\\n###\"\n",
    "    \"Generate the samples keeping these enhanced requirements in mind.\\n###\"\n",
    ")\n",
    "\n",
    "chat_history.append({'role': 'user', 'content': task_description})\n",
    "chat_history.append({'role': 'user', 'content': generate_samples_request})\n",
    "\n",
    "samples_response = chat_with_context(chat_history)\n",
    "chat_history.append({'role': 'assistant', 'content': samples_response})\n",
    "print(\"Generated Samples:\")\n",
    "print(samples_response)\n",
    "\n",
    "# Step 2: Analyze Samples with Chain of Thought\n",
    "analyze_samples_request = (\n",
    "    f\"Based on the following samples: {samples_response}\\n\"\n",
    "    \"Think step by step and derive general principles for classifying news articles into categories 'World', 'Sports', 'Business', or 'Sci/Tech'.\\n\"\n",
    "    \"Avoid focusing on specific details of the provided samples. Instead, develop broader, example-agnostic guidelines that can be applied universally to classify any news article. Conclude your analysis with clear, concise bullet points outlining:\\n\"\n",
    "    \"- The general characteristics that typically define each category.\\n\"\n",
    "    \"- Common mistakes that might lead to misclassifications and how to avoid them.\\n\"\n",
    "    \"- Guidelines under which circumstances each label should be predicted.\\n\"\n",
    "    \"These principles should help in accurately predicting the category of a news article based on its content without additional context.\"\n",
    ")\n",
    "\n",
    "chat_history.append({'role': 'user', 'content': analyze_samples_request})\n",
    "\n",
    "analysis_response = chat_with_context(chat_history)\n",
    "chat_history.append({'role': 'assistant', 'content': analysis_response})\n",
    "print(\"Analysis of Samples:\")\n",
    "print(analysis_response)\n",
    "\n",
    "# Step 3: Generate Optimized Prompt\n",
    "generate_prompt_request = (\n",
    "    f\"Based on the following analysis: {analysis_response}\\nGenerate an optimized prompt for predicting \"\n",
    "    \"whether a news article is 'World', 'Sports', 'Business', or 'Sci/Tech'.\\n\\n\"\n",
    "    \"### Requirements for Optimized Prompt:\\n\"\n",
    "    \"1. The prompt must include a clear description of the task and the labels.\\n\"\n",
    "    \"2. It should provide a comprehensive criteria for classifying news articles as 'World', 'Sports', 'Business', or 'Sci/Tech' based on the analysis.\\n\"\n",
    "    \"3. The prompt must ensure that the model responds strictly with 'World', 'Sports', 'Business', or 'Sci/Tech'.\\n\"\n",
    "    \"4. The prompt should help the model avoid common pitfalls and misclassifications identified during the analysis.\\n\"\n",
    "    \"5. Ensure the language is unambiguous and tailored to maximize the model's prediction accuracy.\"\n",
    "    \"6. Encourage the model to think step by step.\"\n",
    "    \"Respond with no other explanation but only the content of the prompt that is ready for the model to predict\\n\"\n",
    "    \"Prompt:\"\n",
    ")\n",
    "\n",
    "chat_history.append({'role': 'user', 'content': generate_prompt_request})\n",
    "\n",
    "optimized_prompt_response = chat_with_context(chat_history)\n",
    "chat_history.append({'role': 'assistant', 'content': optimized_prompt_response})\n",
    "print(\"Optimized Prompt:\")\n",
    "print(optimized_prompt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: Business\n",
      "\n",
      "Predicted 1 samples out of 10000\n",
      "Predicted Label: World\n",
      "\n",
      "Predicted Label: Sports\n",
      "\n",
      "Predicted 1001 samples out of 10000\n",
      "Predicted 2001 samples out of 10000\n",
      "Predicted 3001 samples out of 10000\n",
      "Predicted 4001 samples out of 10000\n",
      "Predicted 5001 samples out of 10000\n",
      "Predicted 6001 samples out of 10000\n",
      "Predicted 7001 samples out of 10000\n",
      "Predicted 8001 samples out of 10000\n",
      "Predicted 9001 samples out of 10000\n",
      "Model Accuracy: 0.7843\n"
     ]
    }
   ],
   "source": [
    "# Function to predict the label of a given news article\n",
    "def get_prediction(text):\n",
    "    prompt = (\n",
    "        \"Follow the guidelines of the prompt step by step:\\n\"\n",
    "        f\"{optimized_prompt_response}\\n\\n\"\n",
    "        f\"article:{text}\\n\"\n",
    "        \"### Requirements:\\n\"\n",
    "        \"1. Respond with only the label name ('World', 'Sports', 'Business', or 'Sci/Tech').\\n\"\n",
    "        \"2. Do not provide any additional text or explanation.\\n\"\n",
    "        \"Respond with only the label name:\"\n",
    "    )\n",
    "    response = ollama.generate(model='llama3', prompt=prompt)\n",
    "\n",
    "    # Extract the predicted label from the response\n",
    "    prediction = response['response'].strip().replace(\"**\", \"\").replace(\"'\", \"\").replace('\"', '')\n",
    "    # if prediction is not valid, perform the prediction again\n",
    "    if prediction not in ['World', 'Sports', 'Business', 'Sci/Tech']:\n",
    "        prediction = get_prediction(text)\n",
    "    # format the prediction so that if it keeps only 'World', 'Sports', 'Business', or 'Sci/Tech' as output\n",
    "    return prediction\n",
    "\n",
    "predictions = []\n",
    "# Make predictions\n",
    "for i, text in enumerate(texts):\n",
    "    prediction = get_prediction(text)\n",
    "    if i < 3:\n",
    "        print(f\"Predicted Label: {prediction}\\n\")\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Predicted {i+1} samples out of {len(texts)}\")\n",
    "    if i == 10000:\n",
    "        break\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Step 4: Evaluate the Model\n",
    "true_labels = [mapping[label] for label in labels[:len(predictions)]]\n",
    "accuracy = accuracy_score(true_labels[:len(predictions)], predictions)\n",
    "print(f\"Model Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# zero shot COT Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: Business\n",
      "\n",
      "Predicted 1 samples out of 10000\n",
      "Predicted Label: Business\n",
      "\n",
      "Predicted Label: Sports\n",
      "\n",
      "Predicted 1001 samples out of 10000\n",
      "Predicted 2001 samples out of 10000\n",
      "Predicted 3001 samples out of 10000\n",
      "Predicted 4001 samples out of 10000\n",
      "Predicted 5001 samples out of 10000\n",
      "Predicted 6001 samples out of 10000\n",
      "Predicted 7001 samples out of 10000\n",
      "Predicted 8001 samples out of 10000\n",
      "Predicted 9001 samples out of 10000\n",
      "Model Accuracy: 0.7592\n"
     ]
    }
   ],
   "source": [
    "# Function to predict the label of a given news article\n",
    "def get_prediction(text):\n",
    "    prompt = (\n",
    "        f\"Predict the news article into one of following categories: ('World', 'Sports', 'Business', or 'Sci/Tech')\\n\\n\"\n",
    "        \"Let's think step by step\\n\"\n",
    "        f\"article:{text}\\n\"\n",
    "        \"### Requirements:\\n\"\n",
    "        \"1. Respond with only the label name ('World', 'Sports', 'Business', or 'Sci/Tech').\\n\"\n",
    "        \"2. Do not provide any additional text or explanation.\\n\"\n",
    "        \"Respond with only the label name:\"\n",
    "    )\n",
    "    response = ollama.generate(model='llama3', prompt=prompt)\n",
    "\n",
    "    # Extract the predicted label from the response\n",
    "    prediction = response['response'].strip().replace(\"**\", \"\").replace(\"'\", \"\").replace('\"', '')\n",
    "    if prediction not in ['World', 'Sports', 'Business', 'Sci/Tech']:\n",
    "        prediction = \"Invalid Prediction\"\n",
    "    \n",
    "    # format the prediction so that if it keeps only 'World', 'Sports', 'Business', or 'Sci/Tech' as output\n",
    "    return prediction\n",
    "\n",
    "predictions = []\n",
    "# Make predictions\n",
    "for i, text in enumerate(texts):\n",
    "    prediction = get_prediction(text)\n",
    "    if i < 3:\n",
    "        print(f\"Predicted Label: {prediction}\\n\")\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Predicted {i+1} samples out of {len(texts)}\")\n",
    "    if i == 10000:\n",
    "        break\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Step 4: Evaluate the Model\n",
    "true_labels = [mapping[label] for label in labels[:len(predictions)]]\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f\"Model Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few show Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: Business\n",
      "\n",
      "Predicted 1 samples out of 10000\n",
      "Predicted Label: Business\n",
      "\n",
      "Predicted Label: Sports\n",
      "\n",
      "Predicted 1001 samples out of 10000\n",
      "Predicted 2001 samples out of 10000\n",
      "Predicted 3001 samples out of 10000\n",
      "Predicted 4001 samples out of 10000\n",
      "Predicted 5001 samples out of 10000\n",
      "Predicted 6001 samples out of 10000\n",
      "Predicted 7001 samples out of 10000\n",
      "Predicted 8001 samples out of 10000\n",
      "Predicted 9001 samples out of 10000\n",
      "Model Accuracy: 0.7677\n"
     ]
    }
   ],
   "source": [
    "few_shot_texts = dataset[\"train\"][\"text\"][:3]\n",
    "few_shot_labels = dataset[\"train\"][\"label\"][:3]\n",
    "\n",
    "# Function to predict the label of a given news article\n",
    "def get_prediction(text):\n",
    "    prompt = (\n",
    "        f\"Here are few news articles and their categories:\\n\\n\"\n",
    "        f\"1. {few_shot_texts[0]} - {mapping[few_shot_labels[0]]}\\n\"\n",
    "        f\"2. {few_shot_texts[1]} - {mapping[few_shot_labels[1]]}\\n\"\n",
    "        f\"3. {few_shot_texts[2]} - {mapping[few_shot_labels[2]]}\\n\\n\"\n",
    "        \"Based on these examples, predict the category of the following news article into one of the following categories: ('World', 'Sports', 'Business', or 'Sci/Tech')\\n\\n\"\n",
    "        f\"article:{text}\\n\"\n",
    "        \"### Requirements:\\n\"\n",
    "        \"1. Respond with only the label name ('World', 'Sports', 'Business', or 'Sci/Tech').\\n\"\n",
    "        \"2. Do not provide any additional text or explanation.\\n\"\n",
    "        \"Respond with only the label name:\"\n",
    "    )\n",
    "    response = ollama.generate(model='llama3', prompt=prompt)\n",
    "\n",
    "    # Extract the predicted label from the response\n",
    "    prediction = response['response'].strip().replace(\"**\", \"\").replace(\"'\", \"\").replace('\"', '')\n",
    "    if prediction not in ['World', 'Sports', 'Business', 'Sci/Tech']:\n",
    "        prediction = \"Invalid Prediction\"\n",
    "    \n",
    "    # format the prediction so that if it keeps only 'World', 'Sports', 'Business', or 'Sci/Tech' as output\n",
    "    return prediction\n",
    "\n",
    "predictions = []\n",
    "# Make predictions\n",
    "for i, text in enumerate(texts):\n",
    "    prediction = get_prediction(text)\n",
    "    if i < 3:\n",
    "        print(f\"Predicted Label: {prediction}\\n\")\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Predicted {i+1} samples out of {len(texts)}\")\n",
    "    if i == 10000:\n",
    "        break\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Step 4: Evaluate the Model\n",
    "true_labels = [mapping[label] for label in labels[:len(predictions)]]\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f\"Model Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: Business\n",
      "\n",
      "Predicted 1 samples out of 10000\n",
      "Predicted Label: Business\n",
      "\n",
      "Predicted Label: Sports\n",
      "\n",
      "Predicted 1001 samples out of 10000\n",
      "Predicted 2001 samples out of 10000\n",
      "Predicted 3001 samples out of 10000\n",
      "Predicted 4001 samples out of 10000\n",
      "Predicted 5001 samples out of 10000\n",
      "Predicted 6001 samples out of 10000\n",
      "Predicted 7001 samples out of 10000\n",
      "Predicted 8001 samples out of 10000\n",
      "Predicted 9001 samples out of 10000\n",
      "Model Accuracy: 0.7646\n"
     ]
    }
   ],
   "source": [
    "# Function to predict the label of a given news article\n",
    "def get_prediction(text):\n",
    "    prompt = (\n",
    "        f\"Predict the news article into one of following categories: ('World', 'Sports', 'Business', or 'Sci/Tech')\\n\\n\"\n",
    "        f\"article:{text}\\n\"\n",
    "        \"### Requirements:\\n\"\n",
    "        \"1. Respond with only the label name ('World', 'Sports', 'Business', or 'Sci/Tech').\\n\"\n",
    "        \"2. Do not provide any additional text or explanation.\\n\"\n",
    "        \"Respond with only the label name:\"\n",
    "    )\n",
    "    response = ollama.generate(model='llama3', prompt=prompt)\n",
    "\n",
    "    # Extract the predicted label from the response\n",
    "    prediction = response['response'].strip().replace(\"**\", \"\").replace(\"'\", \"\").replace('\"', '')\n",
    "    if prediction not in ['World', 'Sports', 'Business', 'Sci/Tech']:\n",
    "        prediction = \"Invalid Prediction\"\n",
    "    \n",
    "    # format the prediction so that if it keeps only 'World', 'Sports', 'Business', or 'Sci/Tech' as output\n",
    "    return prediction\n",
    "\n",
    "predictions = []\n",
    "# Make predictions\n",
    "for i, text in enumerate(texts):\n",
    "    prediction = get_prediction(text)\n",
    "    if i < 3:\n",
    "        print(f\"Predicted Label: {prediction}\\n\")\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Predicted {i+1} samples out of {len(texts)}\")\n",
    "    if i == 10000:\n",
    "        break\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Step 4: Evaluate the Model\n",
    "true_labels = [mapping[label] for label in labels[:len(predictions)]]\n",
    "accuracy = accuracy_score(true_labels[:len(predictions)], predictions)\n",
    "print(f\"Model Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two step answer retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Samples:\n",
      "Here are five news article samples that present unique challenges for predicting their categories:\n",
      "\n",
      "**Sample 1:**\n",
      "Title: \"New Study Reveals Impact of Climate Change on Global Food Supply\"\n",
      "Text: A recent study published in a leading scientific journal has found that climate change is having a significant impact on global food production, with some regions experiencing up to a 50% decline in crop yields. The findings suggest that urgent action is needed to mitigate the effects of climate change on agriculture.\n",
      "\n",
      "Category Prediction Challenge: This sample challenges predictive models because it combines environmental science (Sci/Tech) with global implications (World), making it difficult to categorize solely as 'World' or 'Sci/Tech'.\n",
      "\n",
      "**Sample 2:**\n",
      "Title: \"Lionel Messi Sets New Record for Most Goals Scored in International Competition\"\n",
      "Text: Argentine football star Lionel Messi has set a new record by scoring his 80th goal in international competition, surpassing the previous record held by Iranian striker Ali Daei. The achievement cements Messi's status as one of the greatest players of all time.\n",
      "\n",
      "Category Prediction Challenge: This sample challenges predictive models because it is purely focused on sports news (Sports), but the language used is similar to that found in other categories (e.g., 'World' or 'Business'), making categorization difficult without careful consideration of context.\n",
      "\n",
      "**Sample 3:**\n",
      "Title: \"Elon Musk's Neuralink Subsidiary Files Patent for Brain-Computer Interface Technology\"\n",
      "Text: Elon Musk's neurotechnology company, Neuralink, has filed a patent application for a brain-computer interface (BCI) technology that enables people to control devices with their thoughts. The innovative technology has the potential to revolutionize the way we interact with computers.\n",
      "\n",
      "Category Prediction Challenge: This sample challenges predictive models because it combines cutting-edge technology (Sci/Tech) with business news (Business), making it difficult to categorize solely as 'Sci/Tech' or 'Business'.\n",
      "\n",
      "**Sample 4:**\n",
      "Title: \"Global Economy Faces Uncertainty as Trade Wars Escalate\"\n",
      "Text: The ongoing trade war between the United States and China has sent shockwaves through global financial markets, sparking concerns about a potential economic downturn. As tensions continue to rise, investors are bracing for impact.\n",
      "\n",
      "Category Prediction Challenge: This sample challenges predictive models because it is primarily focused on global economics (Business), but it also touches on international relations (World) and the potential consequences of the trade war on the global economy, making categorization difficult without careful consideration of context.\n",
      "\n",
      "**Sample 5:**\n",
      "Title: \"NASA's Perseverance Rover Discovers Evidence of Ancient Lake on Mars\"\n",
      "Text: NASA's Perseverance rover has discovered a vast lakebed on Mars that dates back millions of years. The finding provides crucial insights into the planet's ancient habitability and raises questions about the possibility of life existing on Mars in the past.\n",
      "\n",
      "Category Prediction Challenge: This sample challenges predictive models because it is purely focused on space exploration (Sci/Tech), but the language used is similar to that found in other categories (e.g., 'World' or 'Business'), making categorization difficult without careful consideration of context.\n",
      "Analysis of Samples:\n",
      "Based on the provided samples, I have derived general principles for classifying news articles into categories 'World', 'Sports', 'Business', or 'Sci/Tech'. Here are the guidelines:\n",
      "\n",
      "**General Characteristics that Typically Define Each Category:**\n",
      "\n",
      "* **'World':**\n",
      "\t+ Focuses on global events, international relations, and human experiences.\n",
      "\t+ Often involves politics, economics, culture, and social issues.\n",
      "\t+ Can include stories about countries, governments, organizations, and individuals.\n",
      "* **'Sports':**\n",
      "\t+ Primarily focuses on athletic competitions, teams, players, and events.\n",
      "\t+ Typically covers news related to sports leagues, championships, and tournaments.\n",
      "\t+ May involve personalities, scandals, or controversies in the sports world.\n",
      "* **'Business':**\n",
      "\t+ Centers around economic and financial news, industry trends, and corporate activities.\n",
      "\t+ Often discusses companies, markets, investments, trade, and entrepreneurship.\n",
      "\t+ Can include stories about mergers, acquisitions, IPOs, and other business transactions.\n",
      "* **'Sci/Tech':**\n",
      "\t+ Primarily focuses on scientific discoveries, technological advancements, and innovations.\n",
      "\t+ Typically covers news related to research, development, and applications in various fields (e.g., medicine, space exploration).\n",
      "\t+ May involve breakthroughs, patents, and applications of new technologies.\n",
      "\n",
      "**Common Mistakes that Might Lead to Misclassifications:**\n",
      "\n",
      "* Failing to consider the context: Be aware of the story's focus, tone, and language. A story might seem like it belongs to one category at first glance, but further reading reveals a deeper connection to another category.\n",
      "* Ignoring subtleties: Pay attention to subtle clues that can help you determine the correct category. For example, the use of technical terms or jargon in a story may indicate its relevance to 'Sci/Tech', while the presence of sports terminology might suggest it belongs to 'Sports'.\n",
      "* Relying on surface-level information: Don't rely solely on the article's title or introduction. Read the entire piece and consider the main points, supporting details, and overall tone before making a classification decision.\n",
      "\n",
      "**Guidelines for Predicting Each Label:**\n",
      "\n",
      "1. **'World':**\n",
      "\t+ If the story focuses on global events, international relations, or human experiences, predict 'World'.\n",
      "\t+ Consider whether the story involves politics, economics, culture, or social issues.\n",
      "2. **'Sports':**\n",
      "\t+ If the story is primarily about athletic competitions, teams, players, or events, predict 'Sports'.\n",
      "\t+ Check if the story discusses sports leagues, championships, or tournaments.\n",
      "3. **'Business':**\n",
      "\t+ If the story centers around economic and financial news, industry trends, or corporate activities, predict 'Business'.\n",
      "\t+ Consider whether the story involves companies, markets, investments, trade, or entrepreneurship.\n",
      "4. **'Sci/Tech':**\n",
      "\t+ If the story focuses on scientific discoveries, technological advancements, or innovations, predict 'Sci/Tech'.\n",
      "\t+ Check if the story discusses research, development, or applications in various fields (e.g., medicine, space exploration).\n",
      "\n",
      "By following these guidelines and being mindful of common mistakes, you can accurately predict the category of a news article based on its content without additional context.\n",
      "Optimized Prompt:\n",
      "Predict the category of a news article as 'World', 'Sports', 'Business', or 'Sci/Tech' based on its content. Consider the following criteria:\n",
      "\n",
      "* If the story focuses on global events, international relations, human experiences, politics, economics, culture, or social issues, predict 'World'.\n",
      "* If the story is primarily about athletic competitions, teams, players, or events, and discusses sports leagues, championships, or tournaments, predict 'Sports'.\n",
      "* If the story centers around economic and financial news, industry trends, corporate activities, companies, markets, investments, trade, or entrepreneurship, predict 'Business'.\n",
      "* If the story focuses on scientific discoveries, technological advancements, innovations, research, development, or applications in various fields (e.g., medicine, space exploration), predict 'Sci/Tech'.\n",
      "\n",
      "Avoid common mistakes by considering:\n",
      "* Context: Be aware of the story's focus, tone, and language.\n",
      "* Subtleties: Pay attention to subtle clues that can help you determine the correct category.\n",
      "* Surface-level information: Don't rely solely on the article's title or introduction; read the entire piece.\n",
      "\n",
      "Step-by-step, think carefully about each aspect before making a prediction.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Function to interact with the Ollama API\n",
    "def chat_with_context(history):\n",
    "    response = ollama.chat(model='llama3', messages=history)\n",
    "    return response['message']['content']\n",
    "\n",
    "# Initialize the chat history\n",
    "chat_history = []\n",
    "num_samples = 5\n",
    "\n",
    "# Step 1: Define Task Description for AG News Dataset\n",
    "task_description = \"We need to predict the category of a given news article. The labels are 'World', 'Sports', 'Business', and 'Sci/Tech'.\"\n",
    "instruction = \"Classify each news article as 'World', 'Sports', 'Business', or 'Sci/Tech'.\"\n",
    "\n",
    "generate_samples_request = (\n",
    "    f\"As an advanced language model you should create {num_samples} samples for the task outlined below.\\n\"\n",
    "    \"Generate samples that are likely to be correctly classified as 'World', 'Sports', 'Business', or 'Sci/Tech' and samples that might be misclassified according to the task instructions.\\n\\n\"\n",
    "    f\"### Task Description:\\n{task_description}\\n\\n\"\n",
    "    f\"### Task Instructions:\\n{instruction}\\n\\n\"\n",
    "    \"### Requirements for Samples:\\n\"\n",
    "    \"1. Each sample must present a unique and intricate challenge.\\n\"\n",
    "    \"2. The complexity of the samples should be such that simply applying the given task instruction would likely lead to incorrect or incomplete results for some samples.\\n\"\n",
    "    \"3. The samples should cover a diverse range of scenarios within the scope of the task, avoiding repetition and predictability.\\n\"\n",
    "    \"4. Ensure that the samples, while challenging, remain realistic and pertinent to the task's context.\\n\"\n",
    "    \"Generate the samples keeping these requirements in mind.\\n###\"\n",
    ")\n",
    "\n",
    "chat_history.append({'role': 'user', 'content': task_description})\n",
    "chat_history.append({'role': 'user', 'content': generate_samples_request})\n",
    "\n",
    "samples_response = chat_with_context(chat_history)\n",
    "chat_history.append({'role': 'assistant', 'content': samples_response})\n",
    "print(\"Generated Samples:\")\n",
    "print(samples_response)\n",
    "\n",
    "# Step 2: Analyze Samples with Chain of Thought\n",
    "analyze_samples_request = (\n",
    "    f\"Based on the following samples: {samples_response}\\n\"\n",
    "    \"Think step by step and derive general principles for classifying news articles into categories 'World', 'Sports', 'Business', or 'Sci/Tech'.\\n\"\n",
    "    \"Avoid focusing on specific details of the provided samples. Instead, develop broader, example-agnostic guidelines that can be applied universally to classify any news article. Conclude your analysis with clear, concise bullet points outlining:\\n\"\n",
    "    \"- The general characteristics that typically define each category.\\n\"\n",
    "    \"- Common mistakes that might lead to misclassifications and how to avoid them.\\n\"\n",
    "    \"- Guidelines under which circumstances each label should be predicted.\\n\"\n",
    "    \"These principles should help in accurately predicting the category of a news article based on its content without additional context.\"\n",
    ")\n",
    "\n",
    "chat_history.append({'role': 'user', 'content': analyze_samples_request})\n",
    "\n",
    "analysis_response = chat_with_context(chat_history)\n",
    "chat_history.append({'role': 'assistant', 'content': analysis_response})\n",
    "print(\"Analysis of Samples:\")\n",
    "print(analysis_response)\n",
    "\n",
    "# Step 3: Generate Optimized Prompt\n",
    "generate_prompt_request = (\n",
    "    f\"Based on the following analysis: {analysis_response}\\nGenerate an optimized prompt for predicting \"\n",
    "    \"whether a news article is 'World', 'Sports', 'Business', or 'Sci/Tech'. Ensure the model responds only with 'World', 'Sports', 'Business', or 'Sci/Tech'.\\n\\n\"\n",
    "    \"### Requirements for Optimized Prompt:\\n\"\n",
    "    \"1. The prompt must include a clear description of the task and the labels.\\n\"\n",
    "    \"2. It should provide a comprehensive criteria for classifying news articles as 'World', 'Sports', 'Business', or 'Sci/Tech' based on the analysis.\\n\"\n",
    "    \"3. The prompt should help the model avoid common pitfalls and misclassifications identified during the analysis.\\n\"\n",
    "    \"4. Ensure the language is unambiguous and tailored to maximize the model's prediction accuracy.\"\n",
    "    \"5. Encourage the model to think step by step.\"\n",
    "    \"Respond with no other explanation but only the content of the prompt that is ready for the model to predict\\n\"\n",
    "    \"Prompt:\"\n",
    ")\n",
    "\n",
    "chat_history.append({'role': 'user', 'content': generate_prompt_request})\n",
    "\n",
    "optimized_prompt_response = chat_with_context(chat_history)\n",
    "chat_history.append({'role': 'assistant', 'content': optimized_prompt_response})\n",
    "print(\"Optimized Prompt:\")\n",
    "print(optimized_prompt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: World\n",
      "\n",
      "Predicted 1 samples out of 120000\n",
      "Accuracy after 1 samples: 1.0\n",
      "Predicted Label: Sci/Tech\n",
      "\n",
      "Predicted Label: World\n",
      "\n",
      "Predicted 101 samples out of 120000\n",
      "Accuracy after 101 samples: 0.8217821782178217\n",
      "Predicted 201 samples out of 120000\n",
      "Accuracy after 201 samples: 0.8109452736318408\n",
      "Predicted 301 samples out of 120000\n",
      "Accuracy after 301 samples: 0.7940199335548173\n",
      "Predicted 401 samples out of 120000\n",
      "Accuracy after 401 samples: 0.770573566084788\n",
      "Predicted 501 samples out of 120000\n",
      "Accuracy after 501 samples: 0.7504990019960079\n",
      "Predicted 601 samples out of 120000\n",
      "Accuracy after 601 samples: 0.7554076539101497\n",
      "Predicted 701 samples out of 120000\n",
      "Accuracy after 701 samples: 0.7631954350927247\n",
      "Predicted 801 samples out of 120000\n",
      "Accuracy after 801 samples: 0.7715355805243446\n",
      "Predicted 901 samples out of 120000\n",
      "Accuracy after 901 samples: 0.7702552719200888\n",
      "Predicted 1001 samples out of 120000\n",
      "Accuracy after 1001 samples: 0.7752247752247752\n",
      "Predicted 1101 samples out of 120000\n",
      "Accuracy after 1101 samples: 0.7720254314259763\n",
      "Predicted 1201 samples out of 120000\n",
      "Accuracy after 1201 samples: 0.7710241465445462\n",
      "Predicted 1301 samples out of 120000\n",
      "Accuracy after 1301 samples: 0.7732513451191392\n",
      "Predicted 1401 samples out of 120000\n",
      "Accuracy after 1401 samples: 0.7765881513204854\n",
      "Predicted 1501 samples out of 120000\n",
      "Accuracy after 1501 samples: 0.7741505662891406\n",
      "Predicted 1601 samples out of 120000\n",
      "Accuracy after 1601 samples: 0.7795128044971893\n",
      "Predicted 1701 samples out of 120000\n",
      "Accuracy after 1701 samples: 0.7818930041152263\n",
      "Predicted 1801 samples out of 120000\n",
      "Accuracy after 1801 samples: 0.7773459189339256\n",
      "Predicted 1901 samples out of 120000\n",
      "Accuracy after 1901 samples: 0.7769594950026302\n",
      "Predicted 2001 samples out of 120000\n",
      "Accuracy after 2001 samples: 0.775112443778111\n",
      "Predicted 2101 samples out of 120000\n",
      "Accuracy after 2101 samples: 0.7772489290813898\n",
      "Predicted 2201 samples out of 120000\n",
      "Accuracy after 2201 samples: 0.777373920945025\n",
      "Predicted 2301 samples out of 120000\n",
      "Accuracy after 2301 samples: 0.7774880486744894\n",
      "Predicted 2401 samples out of 120000\n",
      "Accuracy after 2401 samples: 0.7780091628488129\n",
      "Predicted 2501 samples out of 120000\n",
      "Accuracy after 2501 samples: 0.7796881247501\n",
      "Predicted 2601 samples out of 120000\n",
      "Accuracy after 2601 samples: 0.7793156478277585\n",
      "Predicted 2701 samples out of 120000\n",
      "Accuracy after 2701 samples: 0.7782302850796001\n",
      "Predicted 2801 samples out of 120000\n",
      "Accuracy after 2801 samples: 0.7779364512674045\n",
      "Predicted 2901 samples out of 120000\n",
      "Accuracy after 2901 samples: 0.7783522923129955\n",
      "Predicted 3001 samples out of 120000\n",
      "Accuracy after 3001 samples: 0.7784071976007997\n",
      "Predicted 3101 samples out of 120000\n",
      "Accuracy after 3101 samples: 0.7765237020316027\n",
      "Predicted 3201 samples out of 120000\n",
      "Accuracy after 3201 samples: 0.7753826929084661\n",
      "Predicted 3301 samples out of 120000\n",
      "Accuracy after 3301 samples: 0.7752196304150257\n",
      "Predicted 3401 samples out of 120000\n",
      "Accuracy after 3401 samples: 0.7759482505145545\n",
      "Predicted 3501 samples out of 120000\n",
      "Accuracy after 3501 samples: 0.7743501856612397\n",
      "Predicted 3601 samples out of 120000\n",
      "Accuracy after 3601 samples: 0.776173285198556\n",
      "Predicted 3701 samples out of 120000\n",
      "Accuracy after 3701 samples: 0.7757362874898676\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(texts):\n\u001b[1;32m---> 36\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mget_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     predictions\u001b[38;5;241m.\u001b[39mappend(prediction)\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m3\u001b[39m:\n",
      "Cell \u001b[1;32mIn[44], line 12\u001b[0m, in \u001b[0;36mget_prediction\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_prediction\u001b[39m(text):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# First part: Get a detailed response following the general guidelines of the prompt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     initial_prompt \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFollow the guidelines of the prompt:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimized_prompt_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvide your detailed analysis and suggest a category based on the content of the article.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m     )\n\u001b[1;32m---> 12\u001b[0m     initial_response \u001b[38;5;241m=\u001b[39m \u001b[43mollama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mllama3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     detailed_analysis \u001b[38;5;241m=\u001b[39m initial_response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# Second part: Narrow down to just the predicted label\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\solow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ollama\\_client.py:126\u001b[0m, in \u001b[0;36mClient.generate\u001b[1;34m(self, model, prompt, system, template, context, stream, raw, format, images, options, keep_alive)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model:\n\u001b[0;32m    124\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m RequestError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmust provide a model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/api/generate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m  \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtemplate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mraw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m_encode_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mformat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkeep_alive\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m  \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m  \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\solow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ollama\\_client.py:97\u001b[0m, in \u001b[0;36mClient._request_stream\u001b[1;34m(self, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_request_stream\u001b[39m(\n\u001b[0;32m     92\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     93\u001b[0m   \u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m     94\u001b[0m   stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     95\u001b[0m   \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     96\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Mapping[\u001b[38;5;28mstr\u001b[39m, Any], Iterator[Mapping[\u001b[38;5;28mstr\u001b[39m, Any]]]:\n\u001b[1;32m---> 97\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32mc:\\Users\\solow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ollama\\_client.py:68\u001b[0m, in \u001b[0;36mClient._request\u001b[1;34m(self, method, url, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, method: \u001b[38;5;28mstr\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m httpx\u001b[38;5;241m.\u001b[39mResponse:\n\u001b[1;32m---> 68\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "File \u001b[1;32mc:\\Users\\solow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpx\\_client.py:827\u001b[0m, in \u001b[0;36mClient.request\u001b[1;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[0;32m    812\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[0;32m    814\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[0;32m    815\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    816\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    825\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[0;32m    826\u001b[0m )\n\u001b[1;32m--> 827\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\solow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpx\\_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[0;32m    906\u001b[0m follow_redirects \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    907\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_redirects\n\u001b[0;32m    908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[0;32m    910\u001b[0m )\n\u001b[0;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[1;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[1;32mc:\\Users\\solow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpx\\_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[1;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[0;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\solow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpx\\_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[1;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    977\u001b[0m     hook(request)\n\u001b[1;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\solow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpx\\_client.py:1015\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m   1010\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1011\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1012\u001b[0m     )\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[1;32m-> 1015\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[0;32m   1019\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[1;32mc:\\Users\\solow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpx\\_transports\\default.py:233\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    220\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[0;32m    221\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    222\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    230\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    231\u001b[0m )\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 233\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[0;32m    238\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[0;32m    239\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    240\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[0;32m    241\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    242\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\solow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    213\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[1;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, Iterable)\n",
      "File \u001b[1;32mc:\\Users\\solow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    192\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[0;32m    204\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[1;32mc:\\Users\\solow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpcore\\_sync\\connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\solow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpcore\\_sync\\http11.py:143\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32mc:\\Users\\solow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpcore\\_sync\\http11.py:113\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[0;32m    106\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    107\u001b[0m     (\n\u001b[0;32m    108\u001b[0m         http_version,\n\u001b[0;32m    109\u001b[0m         status,\n\u001b[0;32m    110\u001b[0m         reason_phrase,\n\u001b[0;32m    111\u001b[0m         headers,\n\u001b[0;32m    112\u001b[0m         trailing_data,\n\u001b[1;32m--> 113\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    115\u001b[0m         http_version,\n\u001b[0;32m    116\u001b[0m         status,\n\u001b[0;32m    117\u001b[0m         reason_phrase,\n\u001b[0;32m    118\u001b[0m         headers,\n\u001b[0;32m    119\u001b[0m     )\n\u001b[0;32m    121\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[1;32mc:\\Users\\solow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpcore\\_sync\\http11.py:186\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    183\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 186\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[0;32m    188\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\solow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpcore\\_sync\\http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    221\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[1;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[1;32mc:\\Users\\solow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpcore\\_backends\\sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[1;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[1;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Function to predict the label of a given news article\n",
    "def get_prediction(text):\n",
    "    # First part: Get a detailed response following the general guidelines of the prompt\n",
    "    initial_prompt = (\n",
    "        \"Follow the guidelines of the prompt:\\n\"\n",
    "        f\"{optimized_prompt_response}\\n\\n\"\n",
    "        f\"Article: {text}\\n\"\n",
    "        \"### Initial Analysis:\\n\"\n",
    "        \"Provide your detailed analysis and suggest a category based on the content of the article.\"\n",
    "    )\n",
    "    \n",
    "    initial_response = ollama.generate(model='llama3', prompt=initial_prompt)\n",
    "    detailed_analysis = initial_response['response'].strip()\n",
    "\n",
    "    # Second part: Narrow down to just the predicted label\n",
    "    final_prompt = (\n",
    "        \"Based on the detailed analysis, respond with only the category name:\\n\"\n",
    "        f\"{detailed_analysis}\\n\"\n",
    "        \"### Requirements:\\n\"\n",
    "        \"1. Respond with only the label name ('World', 'Sports', 'Business', or 'Sci/Tech').\\n\"\n",
    "        \"2. Do not provide any additional text or explanation.\\n\"\n",
    "        \"Respond with only the label name:\"\n",
    "    )\n",
    "\n",
    "    final_response = ollama.generate(model='llama3', prompt=final_prompt)\n",
    "    prediction = final_response['response'].strip().replace(\"**\", \"\").replace(\"'\", \"\").replace('\"', '')\n",
    "    # if prediction is not valid, perform the prediction again\n",
    "    if prediction not in ['World', 'Sports', 'Business', 'Sci/Tech']:\n",
    "        prediction = get_prediction(text)\n",
    "    # format the prediction so that if it keeps only 'World', 'Sports', 'Business', or 'Sci/Tech' as output\n",
    "    return prediction\n",
    "true_labels = [mapping[label] for label in labels]\n",
    "predictions = []\n",
    "# Make predictions\n",
    "for i, text in enumerate(texts):\n",
    "    prediction = get_prediction(text)\n",
    "    predictions.append(prediction)\n",
    "    if i < 3:\n",
    "        print(f\"Predicted Label: {prediction}\\n\")\n",
    "    if i % 100  == 0:\n",
    "        print(f\"Predicted {i+1} samples out of {len(texts)}\")\n",
    "        acc = accuracy_score(true_labels[:len(predictions)], predictions)\n",
    "        print(f\"Accuracy after {i+1} samples: {acc}\")\n",
    "    if i == 10000:\n",
    "        break\n",
    "\n",
    "# Step 4: Evaluate the Model\n",
    "\n",
    "accuracy = accuracy_score(true_labels[:len(predictions)], predictions)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
