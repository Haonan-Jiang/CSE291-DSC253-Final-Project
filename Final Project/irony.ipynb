{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0      1                                                  2\n",
      "0  Tweet index  Label                                         Tweet text\n",
      "1            1      1  Sweet United Nations video. Just in time for C...\n",
      "2            2      1  @mrdahl87 We are rumored to have talked to Erv...\n",
      "3            3      1  Hey there! Nice to see you Minnesota/ND Winter...\n",
      "4            4      0                3 episodes left I'm dying over here\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    0\n",
      "5    1\n",
      "Name: 1, dtype: object\n",
      "1    Sweet United Nations video. Just in time for C...\n",
      "2    @mrdahl87 We are rumored to have talked to Erv...\n",
      "3    Hey there! Nice to see you Minnesota/ND Winter...\n",
      "4                  3 episodes left I'm dying over here\n",
      "5    I can't breathe! was chosen as the most notabl...\n",
      "Name: 2, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "# load from txt file\n",
    "datapath = 'dataset/irony'\n",
    "df = pd.read_csv(datapath+'/SemEval2018-T3-train-taskA.txt', sep='\\t', header=None)\n",
    "print(df.head())\n",
    "labels = df[1]\n",
    "texts = df[2]\n",
    "# remove the first row in labels and texts\n",
    "labels = labels[1:]\n",
    "texts = texts[1:]\n",
    "print(labels.head())\n",
    "print(texts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# learn from various samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Samples:\n",
      "Here are five tweet samples that aim to present unique challenges and cover a diverse range of scenarios:\n",
      "\n",
      "**Sample 1: Double Meaning**\n",
      "\"Just got my period... I mean, just got my package delivered! #newkitchenware\" (Ironic)\n",
      "\n",
      "This sample is designed to test the model's ability to recognize double meanings. The phrase \"got my period\" can refer to both menstruation and receiving a delivery, making it essential for the model to consider the context and intent behind the tweet.\n",
      "\n",
      "**Sample 2: Sarcastic Statement**\n",
      "\"I'm so excited to be stuck in this traffic jam! Who needs to get to work on time, anyway? #rushhourblues\" (Ironic)\n",
      "\n",
      "This sample is meant to challenge the model's ability to detect sarcasm. The statement \"so excited\" is clearly meant to convey the opposite emotion, and the model must recognize the tone as playful rather than genuine.\n",
      "\n",
      "**Sample 3: Hyperbole**\n",
      "\"Just ran a marathon... in my backyard! I'm basically an Olympic athlete now! #fitnessgoals\" (Not Ironic)\n",
      "\n",
      "This sample aims to test the model's ability to distinguish between exaggeration and irony. While the statement is clearly hyperbolic, it doesn't contain any obvious ironic markers, making it necessary for the model to consider the context and the speaker's intentions.\n",
      "\n",
      "**Sample 4: Subtle Humor**\n",
      "\"I love how my cat thinks she owns the place... even when I'm trying to sleep in! #felineoverlords\" (Ironic)\n",
      "\n",
      "This sample is designed to challenge the model's ability to recognize subtle humor. The tweet may not explicitly state that it's ironic, but the speaker's tone and the absurdity of the situation should give away their intention.\n",
      "\n",
      "**Sample 5: Misdirection**\n",
      "\"I'm so glad I don't have to deal with rush hour traffic anymore... since I've been working from home for months now! #WFHlife\" (Not Ironic)\n",
      "\n",
      "This sample is meant to test the model's ability to resist misdirection. The statement \"so glad\" seems positive, but it's actually a straightforward expression of relief rather than irony. The model must ignore the initial tone and focus on the speaker's actual sentiment.\n",
      "\n",
      "These samples aim to provide a diverse set of challenges for the task, from double meanings and sarcasm to hyperbole and subtle humor.\n",
      "Analysis of Samples:\n",
      "Based on the provided samples, I've derived general principles for classifying tweets as 'ironic' or 'not ironic'. Here are the key takeaways:\n",
      "\n",
      "**General Principles:**\n",
      "\n",
      "* **Ironic Tweets:**\n",
      "\t+ Typically contain language that is intentionally contrary to the literal meaning.\n",
      "\t+ May use sarcasm, understatement, or overstatement to convey irony.\n",
      "\t+ Often rely on shared knowledge or cultural references to create the ironic effect.\n",
      "\t+ Can be subtle and require close reading to detect.\n",
      "* **Not Ironic Tweets:**\n",
      "\t+ Typically convey a straightforward or genuine sentiment.\n",
      "\t+ May use hyperbole, exaggeration, or colloquialisms, but these should not be mistaken for irony.\n",
      "\t+ Do not rely on shared knowledge or cultural references to create the effect.\n",
      "\n",
      "**Common Mistakes and How to Avoid Them:**\n",
      "\n",
      "* **Overlooking Context:** Don't assume a tweet is ironic just because it contains a phrase that could be interpreted as ironic. Consider the entire tweet, including surrounding language, tone, and cultural context.\n",
      "* **Misinterpreting Tone:** Be cautious of tweets with a sarcastic or playful tone, as these can often be misinterpreted as ironic. Instead, focus on the speaker's intended meaning and the literal content of the tweet.\n",
      "* **Ignoring Intent:** Don't assume a tweet is ironic just because it contains an unexpected twist. Consider the speaker's intent behind the tweet and whether it's meant to be humorous or thought-provoking.\n",
      "\n",
      "**Guidelines:**\n",
      "\n",
      "* **Categorize as Ironic:**\n",
      "\t+ If a tweet contains language that intentionally contradicts its literal meaning.\n",
      "\t+ If a tweet relies on shared knowledge, cultural references, or subtlety to create an ironic effect.\n",
      "\t+ If the tone of the tweet is playful, sarcastic, or humorous, and it's clear that the speaker intended to be ironic.\n",
      "* **Categorize as Not Ironic:**\n",
      "\t+ If a tweet conveys a straightforward or genuine sentiment without intentionally contradictory language.\n",
      "\t+ If the tone of the tweet is serious, informative, or literal, indicating no intention to be ironic.\n",
      "\t+ If the tweet relies on exaggeration, hyperbole, or colloquialisms, but these are not meant to create an ironic effect.\n",
      "\n",
      "By following these principles and guidelines, you can accurately classify tweets as 'ironic' or 'not ironic' based solely on their content.\n",
      "Optimized Prompt:\n",
      "Predict whether a tweet is 'ironic' or 'not ironic' based on its content, considering the following criteria:\n",
      "\n",
      "Classify as 'ironic' if:\n",
      "* The tweet contains language intentionally contrary to its literal meaning.\n",
      "* The tweet relies on shared knowledge, cultural references, or subtlety to create an ironic effect.\n",
      "* The tone of the tweet is playful, sarcastic, or humorous and it's clear that the speaker intended to be ironic.\n",
      "\n",
      "Classify as 'not ironic' if:\n",
      "* The tweet conveys a straightforward or genuine sentiment without intentionally contradictory language.\n",
      "* The tone of the tweet is serious, informative, or literal indicating no intention to be ironic.\n",
      "* Exaggeration, hyperbole, or colloquialisms are used, but not intended to create an ironic effect.\n",
      "\n",
      "Avoid misclassifying by considering:\n",
      "* The entire tweet, including surrounding language, tone, and cultural context.\n",
      "* The speaker's intent behind the tweet and whether it's meant to be humorous or thought-provoking.\n",
      "\n",
      "Respond with only 'ironic' or 'not ironic'.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "# Function to interact with the Ollama API\n",
    "def chat_with_context(history):\n",
    "    response = ollama.chat(model='llama3', messages=history)\n",
    "    return response['message']['content']\n",
    "\n",
    "# Initialize the chat history\n",
    "chat_history = []\n",
    "\n",
    "# Step 1: Generate Correctly Classified and Misclassified Samples\n",
    "num_samples = 5\n",
    "task_description = \"We need to predict whether a given tweet is ironic or not. The labels are 'ironic' and 'not ironic'.\"\n",
    "\n",
    "generate_samples_request = (\n",
    "    f\"As an advanced language model you should create {num_samples} samples for the task outlined below.\\n\"\n",
    "    \"Generate samples that are likely to be correctly classified as 'ironic' or 'not ironic' and samples that might be misclassified according to the task instructions.\\n\\n\"\n",
    "    f\"### Task Description:\\n{task_description}\\n\\n\"\n",
    "    \"### Requirements for Samples:\\n\"\n",
    "    \"1. Each sample must present a unique and intricate challenge.\\n\"\n",
    "    \"2. The complexity of the samples should be such that simply applying the given task instruction would likely lead to incorrect or incomplete results for some samples.\\n\"\n",
    "    \"3. The samples should cover a diverse range of scenarios within the scope of the task, avoiding repetition and predictability.\\n\"\n",
    "    \"4. Ensure that the samples, while challenging, remain realistic and pertinent to the task's context.\\n\"\n",
    "    \"Generate the samples keeping these requirements in mind.\\n###\"\n",
    ")\n",
    "\n",
    "chat_history.append({'role': 'user', 'content': task_description})\n",
    "chat_history.append({'role': 'user', 'content': generate_samples_request})\n",
    "\n",
    "samples_response = chat_with_context(chat_history)\n",
    "chat_history.append({'role': 'assistant', 'content': samples_response})\n",
    "print(\"Generated Samples:\")\n",
    "print(samples_response)\n",
    "\n",
    "# Step 2: Analyze Samples with Chain of Thought\n",
    "analyze_samples_request = (\n",
    "    f\"Here are some samples: {samples_response}\\n\"\n",
    "    \"Think step by step and derive general principles for classifying tweets as 'ironic' or 'not ironic'.\\n\"\n",
    "    \"Avoid focusing on specific details of the provided samples. Instead, develop broader, example-agnostic guidelines that can be applied universally to classify any news article. Conclude your analysis with clear, concise bullet points outlining:\\n\"\n",
    "    \"- The general characteristics that typically define each category.\\n\"\n",
    "    \"- Common mistakes that might lead to misclassifications and how to avoid them.\\n\"\n",
    "    \"- Guidelines under which circumstances each label should be predicted.\\n\"\n",
    "    \"These principles should help in accurately predicting the category of a tweet based on its content without additional context.\"\n",
    ")\n",
    "\n",
    "chat_history.append({'role': 'user', 'content': analyze_samples_request})\n",
    "\n",
    "analysis_response = chat_with_context(chat_history)\n",
    "chat_history.append({'role': 'assistant', 'content': analysis_response})\n",
    "print(\"Analysis of Samples:\")\n",
    "print(analysis_response)\n",
    "\n",
    "# Step 3: Generate Optimized Prompt\n",
    "generate_prompt_request = (\n",
    "    f\"Based on the following analysis: {analysis_response}\\nGenerate an optimized prompt for predicting \"\n",
    "    \"whether a tweet is 'ironic' or 'not ironic'. Ensure the model responds only with 'ironic' or 'not ironic'.\\n\\n\"\n",
    "    \"### Requirements for Optimized Prompt:\\n\"\n",
    "    \"1. The prompt must include a clear description of the task and the labels.\\n\"\n",
    "    \"2. It should provide criteria for classifying tweets as 'ironic' or 'not ironic' based on the analysis.\\n\"\n",
    "    \"3. The prompt must ensure that the model responds strictly with 'ironic' or 'not ironic'.\\n\"\n",
    "    \"4. The prompt should help the model avoid common pitfalls and misclassifications identified during the analysis.\\n\"\n",
    "    \"5. Ensure the language is unambiguous and tailored to maximize the model's prediction accuracy.\\n\"\n",
    "    \"6. Encourage the model to think step by step.\\n\"\n",
    "    \"Respond with no other explanation but only the content of the prompt that is ready for the model to predict\\n\"\n",
    "    \"Prompt:\"\n",
    ")\n",
    "\n",
    "chat_history.append({'role': 'user', 'content': generate_prompt_request})\n",
    "\n",
    "optimized_prompt_response = chat_with_context(chat_history)\n",
    "chat_history.append({'role': 'assistant', 'content': optimized_prompt_response})\n",
    "print(\"Optimized Prompt:\")\n",
    "print(optimized_prompt_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting tweet 1 out of 3817\n",
      "Predicting tweet 1001 out of 3817\n",
      "Predicting tweet 2001 out of 3817\n",
      "Predicting tweet 3001 out of 3817\n",
      "Model Accuracy: 0.5441446161907257\n"
     ]
    }
   ],
   "source": [
    "def get_prediction(text):\n",
    "    prompt = (\n",
    "        f\"{optimized_prompt_response}\\n\\n\"\n",
    "        f\"Tweet: {text}\\n\\n\"\n",
    "        \"### Requirements:\\n\"\n",
    "        \"1. Respond with only a single-digit (0 for not ironic, 1 for ironic).\\n\"\n",
    "        \"2. Do not provide any additional text or explanation.\\n\"\n",
    "        \"Respond with only '0' or '1':\"\n",
    "    )\n",
    "    response = ollama.generate(model='llama3', prompt=prompt)\n",
    "    # Ensure the response is either '0' or '1'\n",
    "    response_text = response['response'].strip()\n",
    "    if '1' in response_text:\n",
    "        return '1'\n",
    "    else:\n",
    "        return '0'\n",
    "\n",
    "predictions = []\n",
    "for i, text in enumerate(texts):\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Predicting tweet {i+1} out of {len(texts)}\")\n",
    "    # if i == 1000:\n",
    "    #     break\n",
    "    prediction = get_prediction(text)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Step 6: Evaluate Model\n",
    "from sklearn.metrics import accuracy_score\n",
    "predictions = [\"1\" if \"1\" in p else \"0\" for p in predictions]\n",
    "accuracy = accuracy_score(labels[:len(predictions)], predictions)\n",
    "print(f\"Model Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With zero shot COT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting tweet 1 out of 3817\n",
      "Predicting tweet 1001 out of 3817\n",
      "Predicting tweet 2001 out of 3817\n",
      "Predicting tweet 3001 out of 3817\n",
      "Model Accuracy: 0.5375949698716269\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "def get_prediction(text):\n",
    "    prompt = (\n",
    "        \"Predict whether the following tweet is ironic or not:\\n\\n\"\n",
    "        \"Let's think step by step\"\n",
    "        f\"Tweet: {text}\\n\\n\"\n",
    "        \"### Requirements:\\n\"\n",
    "        \"1. Respond with only a single-digit (0 for not ironic, 1 for ironic).\\n\"\n",
    "        \"2. Do not provide any additional text or explanation.\\n\"\n",
    "        \"Respond with only '0' or '1':\"\n",
    "    )\n",
    "    response = ollama.generate(model='llama3', prompt=prompt)\n",
    "    # Ensure the response is either '0' or '1'\n",
    "    response_text = response['response'].strip()\n",
    "    if '1' in response_text:\n",
    "        return '1'\n",
    "    else:\n",
    "        return '0'\n",
    "\n",
    "predictions = []\n",
    "for i, text in enumerate(texts):\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Predicting tweet {i+1} out of {len(texts)}\")\n",
    "    # if i == 1000:\n",
    "    #     break\n",
    "    prediction = get_prediction(text)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Step 6: Evaluate Model\n",
    "from sklearn.metrics import accuracy_score\n",
    "predictions = [\"1\" if \"1\" in p else \"0\" for p in predictions]\n",
    "accuracy = accuracy_score(labels, predictions)\n",
    "print(f\"Model Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with basic prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting tweet 1 out of 3817\n",
      "Predicting tweet 1001 out of 3817\n",
      "Predicting tweet 2001 out of 3817\n",
      "Predicting tweet 3001 out of 3817\n",
      "Model Accuracy: 0.48388787005501704\n"
     ]
    }
   ],
   "source": [
    "def get_prediction(text):\n",
    "    response = ollama.generate(model='llama3', prompt=f\"Predict if the tweet text is ironic or not: {text}. make sure to respond with only the prediction value (0 or 1)\")\n",
    "    return response['response']\n",
    "\n",
    "predictions = []\n",
    "for i, text in enumerate(texts):\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Predicting tweet {i+1} out of {len(texts)}\")\n",
    "    prediction = get_prediction(text)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Step 6: Evaluate Model\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(labels, predictions)\n",
    "print(f\"Model Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# few shot prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting tweet 1001 out of 3817\n",
      "Predicting tweet 2001 out of 3817\n",
      "Predicting tweet 3001 out of 3817\n",
      "Model Accuracy: 0.4884635553224961\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "datapath = 'dataset/irony'\n",
    "df = pd.read_csv(datapath+'/SemEval2018-T3-train-taskA.txt', sep='\\t', header=None)\n",
    "labels = df[1][1:]\n",
    "texts = df[2][1:]\n",
    "\n",
    "# construct few shot learning task\n",
    "few_shot_text = texts[:3]\n",
    "few_shot_labels = labels[:3]\n",
    "\n",
    "# use the first few samples as few shot learning examples\n",
    "prompt = (f\"Here are the first few tweets in the dataset:\\n\\n1. {few_shot_text.iloc[0]} label:{few_shot_labels.iloc[0]}\\n2. {few_shot_text.iloc[1]} label:{few_shot_labels.iloc[1]}\\n3. {few_shot_text.iloc[2]} label:{few_shot_labels.iloc[2]}\\n\\n\"\n",
    "          \"Based on these examples, predict whether the following tweet is ironic or not:\"\n",
    "          \"### Requirements:\\n\"\n",
    "        \"1. Respond with only a single-digit (0 for not ironic, 1 for ironic).\\n\"\n",
    "        \"2. Do not provide any additional text or explanation.\\n\"\n",
    "        \"Respond with only '0' or '1':\"\n",
    ")\n",
    "\n",
    "# Function to interact with the Ollama API\n",
    "def get_prediction(text):\n",
    "    response = ollama.generate(model='llama3', prompt=prompt)\n",
    "    # Ensure the response is either '0' or '1'\n",
    "    response_text = response['response'].strip()\n",
    "    if '1' in response_text:\n",
    "        return '1'\n",
    "    else:\n",
    "        return '0'\n",
    "    \n",
    "# Make predictions\n",
    "predictions = []\n",
    "for i, text in enumerate(texts):\n",
    "    if i == 0 or i == 1 or i == 2:\n",
    "        continue\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Predicting tweet {i+1} out of {len(texts)}\")\n",
    "    prediction = get_prediction(text)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Step 6: Evaluate Model\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(labels[3:], predictions)\n",
    "print(f\"Model Accuracy: {accuracy}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
