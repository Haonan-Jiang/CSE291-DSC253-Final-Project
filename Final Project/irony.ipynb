{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0      1                                                  2\n",
      "0  Tweet index  Label                                         Tweet text\n",
      "1            1      1  Sweet United Nations video. Just in time for C...\n",
      "2            2      1  @mrdahl87 We are rumored to have talked to Erv...\n",
      "3            3      1  Hey there! Nice to see you Minnesota/ND Winter...\n",
      "4            4      0                3 episodes left I'm dying over here\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    0\n",
      "5    1\n",
      "Name: 1, dtype: object\n",
      "1    Sweet United Nations video. Just in time for C...\n",
      "2    @mrdahl87 We are rumored to have talked to Erv...\n",
      "3    Hey there! Nice to see you Minnesota/ND Winter...\n",
      "4                  3 episodes left I'm dying over here\n",
      "5    I can't breathe! was chosen as the most notabl...\n",
      "Name: 2, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "import pandas as pd\n",
    "# load from txt file\n",
    "datapath = 'dataset/irony'\n",
    "df = pd.read_csv(datapath+'/SemEval2018-T3-train-taskA.txt', sep='\\t', header=None)\n",
    "print(df.head())\n",
    "labels = df[1]\n",
    "texts = df[2]\n",
    "# remove the first row in labels and texts\n",
    "labels = labels[1:]\n",
    "texts = texts[1:]\n",
    "print(labels.head())\n",
    "print(texts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# learn from various samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Samples:\n",
      "Here are five samples for the task:\n",
      "\n",
      "**Sample 1:**\n",
      "\"Just had the best time at my aunt's funeral! The food was amazing and I got to see some old friends #blessed\" (Label: ironic)\n",
      "\n",
      "This tweet is a classic example of irony, as the speaker claims to have had a good time at a funeral, which is typically a somber occasion. The use of hashtags like \"#blessed\" adds to the irony.\n",
      "\n",
      "**Sample 2:**\n",
      "\"I'm so grateful for my new job! I get to do the same work as before, but now I have to wear a suit and tie #adulting\" (Label: not ironic)\n",
      "\n",
      "This tweet seems straightforwardly positive, with the speaker expressing gratitude for their new job. However, upon closer inspection, it's clear that they're poking fun at the idea of \"growing up\" and taking on adult responsibilities.\n",
      "\n",
      "**Sample 3:**\n",
      "\"Just won a free trip to Hawaii... in my dreams! Guess I'll just have to keep on dreaming #travelgoals\" (Label: ironic)\n",
      "\n",
      "This tweet appears to be a genuine expression of disappointment, but the use of hashtags like \"#travelgoals\" and the phrase \"in my dreams\" suggest that the speaker is actually being sarcastic about their lack of actual travel opportunities.\n",
      "\n",
      "**Sample 4:**\n",
      "\"I'm so done with politics. It's all just a bunch of nonsense anyway #staywoke\" (Label: not ironic)\n",
      "\n",
      "At first glance, this tweet seems like a genuine expression of frustration with politics. However, the speaker's use of hashtags like \"#staywoke\" suggests that they're actually being sarcastic about their willingness to engage with political discourse.\n",
      "\n",
      "**Sample 5:**\n",
      "\"Just realized I've been saying 'I'm fine' for the past 3 years and nothing has changed... but hey, at least my mental health is stable #selfcare\" (Label: ironic)\n",
      "\n",
      "This tweet appears to be a genuine expression of self-awareness, but the use of hashtags like \"#selfcare\" and the deadpan tone suggest that the speaker is actually being sarcastic about their lack of personal growth.\n",
      "\n",
      "These samples are designed to challenge the task instruction by presenting complex scenarios that require careful consideration to correctly classify as 'ironic' or 'not ironic'.\n",
      "Analysis of Samples:\n",
      "Based on the analysis of the provided samples, I've identified some key characteristics that can help us predict whether a tweet is 'ironic' or 'not ironic'.\n",
      "\n",
      "**Correctly Classified Samples:**\n",
      "\n",
      "1. **Incongruity**: Tweets labeled as 'ironic' often exhibit incongruity between what's expected and what's actually happening. For example, Sample 1 claims to have had a good time at an aunt's funeral, which is an unusual or unexpected occurrence.\n",
      "2. **Sarcasm**: Irony often involves sarcasm, either explicitly (e.g., Sample 3) or implicitly (e.g., Sample 5). Pay attention to language that signals irony, such as hashtags like \"#blessed\" or phrases that have a clear opposite meaning.\n",
      "3. **Playfulness**: Tweets labeled as 'ironic' often exhibit playfulness, using humor or wit to convey the intended message. This can include clever wordplay, unexpected twists, or absurd situations.\n",
      "\n",
      "**Incorrectly Classified Samples:**\n",
      "\n",
      "1. **Straightforward Language**: Sample 2 is a good example of this. The language used is straightforward and genuine-sounding, making it difficult to classify as 'ironic'.\n",
      "2. **Lack of Context**: Sample 4 might be misclassified if we don't consider the context in which politics are often discussed online. In this case, the tone seems genuinely frustrated, but the hashtags like \"#staywoke\" suggest sarcasm.\n",
      "3. **Misdirection**: Sample 5 appears to be a genuine expression of self-awareness at first glance, but the use of hashtags like \"#selfcare\" and the deadpan tone hint at irony.\n",
      "\n",
      "**Procedure for Predicting Irony:**\n",
      "\n",
      "1. **Analyze Language**: Examine the language used in the tweet, looking for signs of sarcasm, incongruity, or playfulness.\n",
      "2. **Consider Context**: Take into account the context in which the tweet is being posted. This might include common online tropes, hashtags, or cultural references that could indicate irony.\n",
      "3. **Evaluate Tone**: Assess the tone of the tweet. If it's playful, deadpan, or uses humor, it may be ironic.\n",
      "\n",
      "**Circumstances for Predicting Each Label:**\n",
      "\n",
      "* 'Ironic':\n",
      "\t+ When language exhibits incongruity, sarcasm, or playfulness.\n",
      "\t+ In situations where what's expected doesn't align with reality (e.g., Sample 1).\n",
      "\t+ When the tone is playful, deadpan, or uses humor.\n",
      "* 'Not Ironic':\n",
      "\t+ When language is straightforward and genuine-sounding (e.g., Sample 2).\n",
      "\t+ In situations where the context suggests a genuine expression of emotions or thoughts.\n",
      "\t+ When the tone is serious, solemn, or lacks playfulness.\n",
      "\n",
      "By following this procedure and considering these characteristics, we can increase our accuracy in predicting whether a tweet is 'ironic' or 'not ironic'.\n",
      "Optimized Prompt:\n",
      "Here's an optimized prompt for predicting whether a tweet is 'ironic' or 'not ironic':\n",
      "\n",
      "**Predict Irony in Tweets**\n",
      "\n",
      "Classify each tweet into one of two categories: **Ironic** or **Not Ironic**.\n",
      "\n",
      "To make your predictions, consider the following criteria:\n",
      "\n",
      "* Does the tweet exhibit incongruity between what's expected and what's actually happening?\n",
      "* Is there evidence of sarcasm, either explicitly (e.g., hashtags) or implicitly (e.g., phrases with opposite meanings)?\n",
      "* Is the tone playful, deadpan, or humorous?\n",
      "\n",
      "When classifying a tweet as **Ironic**, consider situations where:\n",
      "\n",
      "* The language is incongruous with the expected outcome.\n",
      "* The tweet uses humor, sarcasm, or playfulness to convey its intended meaning.\n",
      "\n",
      "On the other hand, when classifying a tweet as **Not Ironic**, consider situations where:\n",
      "\n",
      "* The language is straightforward and genuine-sounding.\n",
      "* The context suggests a genuine expression of emotions or thoughts.\n",
      "* The tone is serious, solemn, or lacks playfulness.\n",
      "\n",
      "**Important Notes:**\n",
      "\n",
      "* Avoid misclassifications by considering the context in which the tweet was posted (e.g., online tropes, hashtags, cultural references).\n",
      "* Be cautious when classifying tweets with straightforward language; they may still be ironic if the context suggests otherwise.\n",
      "* Strictly respond with **Ironic** or **Not Ironic**, avoiding any intermediate labels.\n",
      "\n",
      "By following these guidelines, you'll be able to accurately predict whether a tweet is 'ironic' or 'not ironical'.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "# Function to interact with the Ollama API\n",
    "def chat_with_context(history):\n",
    "    response = ollama.chat(model='llama3', messages=history)\n",
    "    return response['message']['content']\n",
    "\n",
    "# Initialize the chat history\n",
    "chat_history = []\n",
    "\n",
    "# Step 1: Generate Correctly Classified and Misclassified Samples\n",
    "num_samples = 5\n",
    "task_description = \"We need to predict whether a given tweet is ironic or not. The labels are 'ironic' and 'not ironic'.\"\n",
    "instruction = \"Classify each tweet as either 'ironic' or 'not ironic'.\"\n",
    "\n",
    "generate_samples_request = (\n",
    "    f\"As an advanced language model you should create {num_samples} samples for the task outlined below.\\n\"\n",
    "    \"Generate samples that are likely to be correctly classified as 'ironic' or 'not ironic' and samples that might be misclassified according to the task instructions.\\n\\n\"\n",
    "    f\"### Task Description:\\n{task_description}\\n\\n\"\n",
    "    f\"### Task Instructions:\\n{instruction}\\n\\n\"\n",
    "    \"### Requirements for Samples:\\n\"\n",
    "    \"1. Each sample must present a unique and intricate challenge.\\n\"\n",
    "    \"2. The complexity of the samples should be such that simply applying the given task instruction would likely lead to incorrect or incomplete results for some samples.\\n\"\n",
    "    \"3. The samples should cover a diverse range of scenarios within the scope of the task, avoiding repetition and predictability.\\n\"\n",
    "    \"4. Ensure that the samples, while challenging, remain realistic and pertinent to the task's context.\\n\"\n",
    "    \"Generate the samples keeping these requirements in mind.\\n###\"\n",
    ")\n",
    "\n",
    "chat_history.append({'role': 'user', 'content': task_description})\n",
    "chat_history.append({'role': 'user', 'content': generate_samples_request})\n",
    "\n",
    "samples_response = chat_with_context(chat_history)\n",
    "chat_history.append({'role': 'assistant', 'content': samples_response})\n",
    "print(\"Generated Samples:\")\n",
    "print(samples_response)\n",
    "\n",
    "# Step 2: Analyze Samples with Chain of Thought\n",
    "analyze_samples_request = (\n",
    "    f\"Here are some samples: {samples_response}\\nUsing chain of thought, analyze these samples \"\n",
    "    \"and conclude a procedure for predicting whether a tweet is 'ironic' or 'not ironic'. Identify key characteristics of both correctly and incorrectly classified samples, capture the mistakes from failed cases, and conclude under what circumstances we should predict each label.\"\n",
    ")\n",
    "\n",
    "chat_history.append({'role': 'user', 'content': analyze_samples_request})\n",
    "\n",
    "analysis_response = chat_with_context(chat_history)\n",
    "chat_history.append({'role': 'assistant', 'content': analysis_response})\n",
    "print(\"Analysis of Samples:\")\n",
    "print(analysis_response)\n",
    "\n",
    "# Step 3: Generate Optimized Prompt\n",
    "generate_prompt_request = (\n",
    "    f\"Based on the following analysis: {analysis_response}\\nGenerate an optimized prompt for predicting \"\n",
    "    \"whether a tweet is 'ironic' or 'not ironic'. Ensure the model responds only with 'ironic' or 'not ironic'.\\n\\n\"\n",
    "    \"### Requirements for Optimized Prompt:\\n\"\n",
    "    \"1. The prompt must include a clear description of the task and the labels.\\n\"\n",
    "    \"2. It should provide criteria for classifying tweets as 'ironic' or 'not ironic' based on the analysis.\\n\"\n",
    "    \"3. The prompt must ensure that the model responds strictly with 'ironic' or 'not ironic'.\\n\"\n",
    "    \"4. The prompt should help the model avoid common pitfalls and misclassifications identified during the analysis.\\n\"\n",
    "    \"5. Ensure the language is unambiguous and tailored to maximize the model's prediction accuracy.\"\n",
    ")\n",
    "\n",
    "chat_history.append({'role': 'user', 'content': generate_prompt_request})\n",
    "\n",
    "optimized_prompt_response = chat_with_context(chat_history)\n",
    "chat_history.append({'role': 'assistant', 'content': optimized_prompt_response})\n",
    "print(\"Optimized Prompt:\")\n",
    "print(optimized_prompt_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting tweet 1 out of 3817\n",
      "Predicting tweet 1001 out of 3817\n",
      "Predicting tweet 2001 out of 3817\n",
      "Predicting tweet 3001 out of 3817\n",
      "Model Accuracy: 0.5279014933193608\n"
     ]
    }
   ],
   "source": [
    "def get_prediction(text):\n",
    "    prompt = (\n",
    "        f\"{optimized_prompt_response}\\n\\n\"\n",
    "        f\"Tweet: {text}\\n\\n\"\n",
    "        \"### Requirements:\\n\"\n",
    "        \"1. Respond with only a single-digit (0 for not ironic, 1 for ironic).\\n\"\n",
    "        \"2. Do not provide any additional text or explanation.\\n\"\n",
    "        \"Respond with only '0' or '1':\"\n",
    "    )\n",
    "    response = ollama.generate(model='llama3', prompt=prompt)\n",
    "    # Ensure the response is either '0' or '1'\n",
    "    response_text = response['response'].strip()\n",
    "    if '1' in response_text:\n",
    "        return '1'\n",
    "    else:\n",
    "        return '0'\n",
    "\n",
    "predictions = []\n",
    "for i, text in enumerate(texts):\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Predicting tweet {i+1} out of {len(texts)}\")\n",
    "    # if i == 1000:\n",
    "    #     break\n",
    "    prediction = get_prediction(text)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Step 6: Evaluate Model\n",
    "from sklearn.metrics import accuracy_score\n",
    "predictions = [\"1\" if \"1\" in p else \"0\" for p in predictions]\n",
    "accuracy = accuracy_score(labels, predictions)\n",
    "print(f\"Model Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With zero shot COT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting tweet 1 out of 3817\n",
      "Predicting tweet 1001 out of 3817\n",
      "Predicting tweet 2001 out of 3817\n",
      "Predicting tweet 3001 out of 3817\n",
      "Model Accuracy: 0.5391668849882106\n"
     ]
    }
   ],
   "source": [
    "def get_prediction(text):\n",
    "    prompt = (\n",
    "        \"Predict whether the following tweet is ironic or not:\\n\\n\"\n",
    "        \"Let's think step by step\"\n",
    "        f\"Tweet: {text}\\n\\n\"\n",
    "        \"### Requirements:\\n\"\n",
    "        \"1. Respond with only a single-digit (0 for not ironic, 1 for ironic).\\n\"\n",
    "        \"2. Do not provide any additional text or explanation.\\n\"\n",
    "        \"Respond with only '0' or '1':\"\n",
    "    )\n",
    "    response = ollama.generate(model='llama3', prompt=prompt)\n",
    "    # Ensure the response is either '0' or '1'\n",
    "    response_text = response['response'].strip()\n",
    "    if '1' in response_text:\n",
    "        return '1'\n",
    "    else:\n",
    "        return '0'\n",
    "\n",
    "predictions = []\n",
    "for i, text in enumerate(texts):\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Predicting tweet {i+1} out of {len(texts)}\")\n",
    "    # if i == 1000:\n",
    "    #     break\n",
    "    prediction = get_prediction(text)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Step 6: Evaluate Model\n",
    "from sklearn.metrics import accuracy_score\n",
    "predictions = [\"1\" if \"1\" in p else \"0\" for p in predictions]\n",
    "accuracy = accuracy_score(labels, predictions)\n",
    "print(f\"Model Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# learn only negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Misleading Samples:\n",
      "Here are 5 misleading samples with likely mispredicted labels:\n",
      "\n",
      "**Sample 1:** \"Just had the best day ever! Nothing can bring me down\" **Likely Mispredicted Label:** 'ironic'\n",
      "\n",
      "This tweet is actually expressing genuine happiness, but its over-the-top language might lead a model to incorrectly predict it's ironic.\n",
      "\n",
      "**Sample 2:** \"I'm so excited for my boring Monday at work\" **Likely Mispredicted Label:** 'not ironic'\n",
      "\n",
      "In reality, the speaker is being sarcastic and ironic, saying they're looking forward to something that sounds unpleasant. A model might mispredict this as not ironic due to the unexpected twist.\n",
      "\n",
      "**Sample 3:** \"Just got my dream job offer! It's every college student's worst nightmare\" **Likely Mispredicted Label:** 'not ironic'\n",
      "\n",
      "This tweet is actually expressing irony, as the speaker is being sarcastic about a positive event. A model might mispredict this as not ironic due to the unexpected negative spin.\n",
      "\n",
      "**Sample 4:** \"I'm so glad I got caught speeding again. It's always fun to spend more money\" **Likely Mispredicted Label:** 'ironic'\n",
      "\n",
      "This tweet is actually expressing genuine frustration, but its tone and language might lead a model to incorrectly predict it's ironic.\n",
      "\n",
      "**Sample 5:** \"Just realized I've been using the same password for all my accounts. Who needs security, anyway?\" **Likely Mispredicted Label:** 'not ironic'\n",
      "\n",
      "In reality, the speaker is being sarcastic and ironic, saying they don't care about security despite the obvious risks. A model might mispredict this as not ironic due to the unexpected seriousness of the situation.\n",
      "\n",
      "These samples are designed to be challenging and likely to cause mispredictions, while still being realistic and relevant to the task.\n",
      "Analysis of Misleading Samples:\n",
      "After analyzing the misleading samples, I've identified some common patterns that may lead to mispredictions:\n",
      "\n",
      "* **Over-the-top language**: Samples 1 and 4 use exaggerated language (\"best day ever\", \"always fun to spend more money\"), which might cause a model to incorrectly predict irony due to the unexpected tone or emphasis.\n",
      "* **Unconventional phrasing**: Samples 2, 3, and 5 use unusual sentence structures or phrases (\"I'm so excited for my boring Monday at work\", \"It's every college student's worst nightmare\", \"Who needs security, anyway?\"), which might throw off a model's predictions. The unexpected twists or word choices may lead to misclassification.\n",
      "* **Misaligned expectations**: Samples 2 and 5 have sarcastic or ironic statements that don't align with the expected tone or sentiment of the tweet (e.g., expecting something negative, but finding something positive). This mismatch can cause a model to mispredict the irony.\n",
      "\n",
      "To improve performance on this task, it's essential to develop models that are robust against these patterns and can accurately identify irony in tweets. This might involve:\n",
      "\n",
      "* Fine-tuning language understanding capabilities to recognize over-the-top language, unconventional phrasing, and misaligned expectations.\n",
      "* Incorporating more nuanced sentiment analysis and context-awareness to better detect irony.\n",
      "* Utilizing large-scale datasets with diverse tweet styles and irony types to train models that can generalize well.\n",
      "\n",
      "By addressing these challenges and improving model robustness, we can enhance the accuracy of irony detection in tweets.\n",
      "Optimized Prompt:\n",
      "Here is an optimized prompt for predicting whether a tweet is 'ironic' or 'not ironic':\n",
      "\n",
      "**Irony Detection Prompt**\n",
      "\n",
      "Task: Classify tweets as 'ironic' or 'not ironic'\n",
      "\n",
      "Labels:\n",
      "\n",
      "* 'irrelevant' (do not respond with this label)\n",
      "* 'ironic'\n",
      "* 'not ironic'\n",
      "\n",
      "Criteria for classification:\n",
      "\n",
      "1. **Tone and Emphasis**: Identify tweets that use over-the-top language, sarcasm, or unexpected twists in tone or emphasis to convey irony.\n",
      "2. **Unconventional Phrasing**: Recognize tweets with unusual sentence structures, word choices, or phrasing that intentionally subvert expectations or create a sense of surprise.\n",
      "3. **Misaligned Expectations**: Detect tweets where the sentiment or tone contradicts what is expected based on the context, making it ironic.\n",
      "\n",
      "Avoid common pitfalls and misclassifications:\n",
      "\n",
      "* Be cautious when encountering over-the-top language or exaggerated expressions, as they may be genuine rather than ironic.\n",
      "* Recognize sarcasm and irony in unexpected places, such as in phrases that seem out of character with the rest of the tweet.\n",
      "* Don't confuse irony with mere surprise or contradiction; ensure that the tweet's tone and sentiment align with its intended meaning.\n",
      "\n",
      "Respond strictly with:\n",
      "\n",
      "* 'ironic' if the tweet exhibits irony based on the above criteria\n",
      "* 'not ironic' if the tweet does not exhibit irony\n",
      "\n",
      "By following this prompt, the model will be equipped to accurately detect irony in tweets while avoiding common pitfalls and misclassifications.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Function to interact with the Ollama API\n",
    "def chat_with_context(history):\n",
    "    response = ollama.chat(model='llama3', messages=history)\n",
    "    return response['message']['content']\n",
    "\n",
    "# Initialize the chat history\n",
    "chat_history = []\n",
    "\n",
    "# Step 1: Define Task Description\n",
    "task_description = \"We need to predict whether a given tweet is ironic or not. The labels are 'ironic' and 'not ironic'.\"\n",
    "chat_history.append({'role': 'user', 'content': task_description})\n",
    "\n",
    "# Step 2: Generate Misleading Samples\n",
    "num_samples = 5\n",
    "generate_misleading_samples_request = (\n",
    "    f\"As an advanced language model, generate {num_samples} misleading samples for the task outlined below.\\n\"\n",
    "    \"For each sample, provide the likely mispredicted label ('ironic' or 'not ironic').\\n\\n\"\n",
    "    f\"### Task Description:\\n{task_description}\\n\\n\"\n",
    "    \"### Requirements for Misleading Samples:\\n\"\n",
    "    \"1. Each sample must be challenging and likely to cause mispredictions.\\n\"\n",
    "    \"2. Include a likely mispredicted label for each sample.\\n\"\n",
    "    \"3. Ensure the samples are realistic and relevant to the task.\\n\"\n",
    "    \"Generate the misleading samples keeping these requirements in mind.\\n###\"\n",
    ")\n",
    "\n",
    "chat_history.append({'role': 'user', 'content': generate_misleading_samples_request})\n",
    "\n",
    "misleading_samples_response = chat_with_context(chat_history)\n",
    "chat_history.append({'role': 'assistant', 'content': misleading_samples_response})\n",
    "print(\"Generated Misleading Samples:\")\n",
    "print(misleading_samples_response)\n",
    "\n",
    "# Step 3: Analyze Misleading Samples\n",
    "analyze_misleading_samples_request = (\n",
    "    f\"Here are some misleading samples: {misleading_samples_response}\\nUsing a step-by-step approach, analyze these samples \"\n",
    "    \"to understand why the mispredictions occur. Conclude the findings in a few bullet points.\"\n",
    ")\n",
    "\n",
    "chat_history.append({'role': 'user', 'content': analyze_misleading_samples_request})\n",
    "\n",
    "analysis_response = chat_with_context(chat_history)\n",
    "chat_history.append({'role': 'assistant', 'content': analysis_response})\n",
    "print(\"Analysis of Misleading Samples:\")\n",
    "print(analysis_response)\n",
    "\n",
    "# Step 4: Generate Optimized Prompt\n",
    "generate_prompt_request = (\n",
    "    f\"Based on the following analysis: {analysis_response}\\nGenerate an optimized prompt for predicting \"\n",
    "    \"whether a tweet is 'ironic' or 'not ironic'. Ensure the model responds only with 'ironic' or 'not ironic'.\\n\\n\"\n",
    "    \"### Requirements for Optimized Prompt:\\n\"\n",
    "    \"1. The prompt must include a clear description of the task and the labels.\\n\"\n",
    "    \"2. It should provide criteria for classifying tweets as 'ironic' or 'not ironic' based on the analysis.\\n\"\n",
    "    \"3. The prompt must ensure that the model responds strictly with 'ironic' or 'not ironic'.\\n\"\n",
    "    \"4. The prompt should help the model avoid common pitfalls and misclassifications identified during the analysis.\\n\"\n",
    "    \"5. Ensure the language is unambiguous and tailored to maximize the model's prediction accuracy.\"\n",
    ")\n",
    "\n",
    "chat_history.append({'role': 'user', 'content': generate_prompt_request})\n",
    "\n",
    "optimized_prompt_response = chat_with_context(chat_history)\n",
    "chat_history.append({'role': 'assistant', 'content': optimized_prompt_response})\n",
    "print(\"Optimized Prompt:\")\n",
    "print(optimized_prompt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting text 1 out of 3817\n",
      "Predicting text 1001 out of 3817\n",
      "Predicting text 2001 out of 3817\n",
      "Predicting text 3001 out of 3817\n",
      "Model Accuracy: 0.5197799318836783\n"
     ]
    }
   ],
   "source": [
    "# Example of using the optimized prompt for prediction\n",
    "def get_prediction(text):\n",
    "    prompt = (\n",
    "        f\"{optimized_prompt_response}\\n\\n\"\n",
    "        f\"Tweet: {text}\\n\\n\"\n",
    "        \"### Requirements:\\n\"\n",
    "        \"1. Respond with only a single-digit (0 for not ironic, 1 for ironic).\\n\"\n",
    "        \"2. Do not provide any additional text or explanation.\\n\"\n",
    "        \"Respond with only '0' or '1':\"\n",
    "    )\n",
    "    response = ollama.generate(model='llama3', prompt=prompt)\n",
    "    # Ensure the response is either '0' or '1'\n",
    "    response_text = response['response'].strip()\n",
    "    if '1' in response_text:\n",
    "        return '1'\n",
    "    else:\n",
    "        return '0'\n",
    "\n",
    "# Make predictions\n",
    "predictions = []\n",
    "for i, text in enumerate(texts):\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Predicting text {i+1} out of {len(texts)}\")\n",
    "    # if i == 1000:\n",
    "    #     break\n",
    "    prediction = get_prediction(text)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Evaluate Model\n",
    "predictions = [\"1\" if \"1\" in p else \"0\" for p in predictions]\n",
    "accuracy = accuracy_score(labels, predictions)\n",
    "print(f\"Model Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with basic prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting tweet 1 out of 3817\n",
      "Predicting tweet 1001 out of 3817\n",
      "Predicting tweet 2001 out of 3817\n",
      "Predicting tweet 3001 out of 3817\n",
      "Model Accuracy: 0.48388787005501704\n"
     ]
    }
   ],
   "source": [
    "def get_prediction(text):\n",
    "    response = ollama.generate(model='llama3', prompt=f\"Predict if the tweet text is ironic or not: {text}. make sure to respond with only the prediction value (0 or 1)\")\n",
    "    return response['response']\n",
    "\n",
    "predictions = []\n",
    "for i, text in enumerate(texts):\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Predicting tweet {i+1} out of {len(texts)}\")\n",
    "    prediction = get_prediction(text)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Step 6: Evaluate Model\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(labels, predictions)\n",
    "print(f\"Model Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# few shot prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting tweet 1001 out of 3817\n",
      "Predicting tweet 2001 out of 3817\n",
      "Predicting tweet 3001 out of 3817\n",
      "Model Accuracy: 0.4884635553224961\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "datapath = 'dataset/irony'\n",
    "df = pd.read_csv(datapath+'/SemEval2018-T3-train-taskA.txt', sep='\\t', header=None)\n",
    "labels = df[1][1:]\n",
    "texts = df[2][1:]\n",
    "\n",
    "# construct few shot learning task\n",
    "few_shot_text = texts[:3]\n",
    "few_shot_labels = labels[:3]\n",
    "\n",
    "# use the first few samples as few shot learning examples\n",
    "prompt = (f\"Here are the first few tweets in the dataset:\\n\\n1. {few_shot_text.iloc[0]} label:{few_shot_labels.iloc[0]}\\n2. {few_shot_text.iloc[1]} label:{few_shot_labels.iloc[1]}\\n3. {few_shot_text.iloc[2]} label:{few_shot_labels.iloc[2]}\\n\\n\"\n",
    "          \"Based on these examples, predict whether the following tweet is ironic or not:\"\n",
    "          \"### Requirements:\\n\"\n",
    "        \"1. Respond with only a single-digit (0 for not ironic, 1 for ironic).\\n\"\n",
    "        \"2. Do not provide any additional text or explanation.\\n\"\n",
    "        \"Respond with only '0' or '1':\"\n",
    ")\n",
    "\n",
    "# Function to interact with the Ollama API\n",
    "def get_prediction(text):\n",
    "    response = ollama.generate(model='llama3', prompt=prompt)\n",
    "    # Ensure the response is either '0' or '1'\n",
    "    response_text = response['response'].strip()\n",
    "    if '1' in response_text:\n",
    "        return '1'\n",
    "    else:\n",
    "        return '0'\n",
    "    \n",
    "# Make predictions\n",
    "predictions = []\n",
    "for i, text in enumerate(texts):\n",
    "    if i == 0 or i == 1 or i == 2:\n",
    "        continue\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Predicting tweet {i+1} out of {len(texts)}\")\n",
    "    prediction = get_prediction(text)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Step 6: Evaluate Model\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(labels[3:], predictions)\n",
    "print(f\"Model Accuracy: {accuracy}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
